In the previous post [http://loveofdatascience.blogspot.com/2023/06/an-introduction-to-prompt-engineering.html] about prompt engineering, I demonstrated that ChatGPT could navigate word math problems if prompted suitably. In this post, I explore ChatGPT's ability to reason about tabular data, including relational database tables.

In the DeepLearning.AI short course on prompt engineering [https://learn.deeplearning.ai/chatgpt-prompt-eng/lesson/1/introduction], a pizza order chatbot was presented. The chatbot was able to answer questions about the menu, gather the order, and summarize the whole thing at the end, complete with the correct total price. The menu in this example was represented in plain text within a system role message (Figure 1).

[fig1.png - chatbot prompt and pizza menu]

Figure 2 shows an example of a complete order executed by the bot.

[fig2.png - sample order]

One well-known problem with chatbots, however, is their tendency to hallucinate [link?]. Asking about product details not known to the model cause it to start making up details. This is dangerous in the case of ChatGPT because the responses can sound reasonably while also being inaccurate.  Figure 3 shows examples of hallucinations about details not provided in the pizza menu. 

[fig3.png - example of water hallucination]

Fixable by specifying (12 ounce...) but problem still exists.

The errors in these specific examples can be addressed by editing the system role prompt with e.g., the actual size of the bottled water. However, fortifying such a menu with every possible detail that might be asked would dramatically increase the size of the prompt. ChatGPT usage fees are based on the number of text tokens the model needs to process, so this will become expensive and is not necessarily fool proof.

One way to make tabular data query more reliable...

[examine if langchain db query of a more structured table is more robust. prototype table, have chatgpt write the query, then stuff the query results back into the prompt for the model to compose an answer to, as in the building systems example; difference here is that langchain db query is being used]

That the chatbot can reason in such detail about the menu, which is essentially a simple table, suggests the potential for reasoning about more complex table or database of tables. 


Does using real relational database tables somehow alleviate the hallucination problem? Does heavily structuring the data prevent wrong details from being made up, e.g., because the answer must be expressable as a valid sql query?

Suggestive that it can retrieve and summarize tablular data.

Show examples for pizza prompt.

Show examples where it is making things up when details are missing.
