In the previous post http://loveofdatascience.blogspot.com/2023/06/an-introduction-to-prompt-engineering.html about prompt engineering for large language models (LLMs), I demonstrated that ChatGPT could navigate word math problems if prompted suitably. In this post, I explore ChatGPT's ability to reason about tabular data, including relational database tables. The source code and raw materials used to build this article are available here https://github.com/tweinzirl/chatGPT_practice/tree/master/blog2 .

# Pizza Time! (No Mushrooms Please)

The DeepLearning.AI short course on prompt engineering https://learn.deeplearning.ai/chatgpt-prompt-eng/lesson/1/introduction culminates with the demo of a pizza order chatbot. The chatbot is able to answer questions about the menu, gather the order, and summarize the whole thing at the end, complete with the correct total price. The menu in this example is represented in plain text within a system role message (Figure 1).

[fig1.png - chatbot prompt and pizza menu]

Figure 2 shows an example of a complete order executed by the bot.

[fig2.png - sample order]

One well-known problem with LLMs, however, is their tendency to hallucinate https://machinelearningmastery.com/a-gentle-introduction-to-hallucinations-in-large-language-models. Asking about product details not known to the model cause it to start making up details. This is generally dangerous because even LLM inaccurate responses can sound reasonable. Figure 3 shows examples of hallucinations about details not provided in the pizza menu. 

[fig3.png - example of water hallucination]

The errors in these specific examples are fixable by adding more details (diameters of pizzas, sizes of drinks) to the menu text in the prompt. Fool-proofing the menu would be an iterative process that could potentially cause regressions as menu complexity increases.


# Querying structured data

In the above example, ChatGPT was able to reason about the menu surprisingly well. Here, I explore if hallucinations are reduced when the menu is structured in SQL tables instead of unstructured text. I hypothesize that well-chosen table name and column names would be used as metadata to help the LLM understand how and whether to answer a given question.

To set up the database, dataframes were made for each each category of item (pizza, topping, side item, drink). They were first prototyped as pandas dataframes and then loaded into a SQLite https://www.sqlite.org/index.html database with SQLAlchemy https://www.sqlalchemy.org/. The table schemas consist of the item name, price, and item description. Figure 4 shows the full content of the tables.

[Figure 4 - Pizza menu]

[intro to langchain and how to set up chain]

ChatGPT was connected to the database via LangChain https://python.langchain.com/docs/get_started/introduction.html , an exciting new project that provides components for a) connecting LLMs to external data sources and b) accomplishing higher level tasks like as Q&A sessions.

LangChain provides a SQLDatabaseChain component https://python.langchain.com/docs/modules/chains/popular/sqlite that is pre-configured to query data in any database that is compatible with SQLAlchemy. The tool implements established best practices https://arxiv.org/abs/2204.00498 for prompting an LLM to write SQL queries, although futher customization of the prompt is possible. The minimal steps for setting up the SQLDatabaseChain include connecting to the database and initializing an LLM https://python.langchain.com/docs/modules/model_io/models/ (Figure 5).  After experimenting with my own prompt, I adopted the default prompt.

[fig5 - set up db chain and view custom prompt]

I expect the SQL chain to perform at least as well as the previous implementation, so I start by presenting the three inputs that caused hallucinations in Figure 3. Thankfully [!], the SQL chain answers all the questions correctly and without hallucinating false details. Figure 6 shows the results. Some of the output remarks (e.g., "The sizes and their corresponding dimensions are not provided in the database.") are not necessarily things that should be shown to end users, and instructing the model to use a different tone would be worthwhile.

[fig6 - three examples of prior hallucinations fixed]

Some comments are in order: [bullet list]

These examples demonstrate the ability of the LLM to examine the data schema and write the SQL needed to efficiently solve the problem. In the first example (how many sizes of pizza and how big they are), the LLM inferred from the Price_Large, Price_Medium, and Price_Small fields that there the three pizza sizes are Large, Medium, and Small. It did not confuse the prices as measurements of size, and it correctly stated that information of dimensions is not available.

In the second example (number and sizes of bottled water), the LLM found only one row and read from the description that the size is 20 ounces. The Price_Medium and Price_Small columns where NULL, but it is not clear this was used in the answer. Asking a related question on the number, size, and cost of Coke returns a detailed and correct answer, which suggests the LLM did not find the answer by accident.

In the third case (Greek salad as a topping), the LLM joined the <it>pizzas</it> and <it>toppings</it> tables together, found no rows where 'Greek salad' was a topping, and concluded it was not possible

As impressive as the above results are, the LLM can fool itself on seemingly simple questions. When asked simply about the diameters of pizza size (which was answered correctly above as part of a longer question), the LLM confuses the price values for pizza dimensions. When asked a second time and reminded that the price values are dollars and not dimensions, it correctly reports that it cannot answer the question.

[fig7 - confusion about pizza diameter]

 This demonstrates a brittleness in the text-to-SQL capabilities of ChatGPT.

[expect db chain to perform at least as well as previous implementation based on the text-based menu. So check if the three hallucination examples are improved. Then try other more complicated examples to show off. comment that SQLDatabaseSequentialChain adds extra layer where the right tables to check are first examined.]

[need to introduce langchain, mention capabilities with prompts and chains, db connections, ]

First try, which failed, make one big table analogous to what is in the above prompt (Figure 1). Having Item Name and Type in one table is confusing because it queries Type for input item name.

 questions to test:
  - How many sizes of pizza do you have?  Name them.
  - What drinks can I order?
  - Please tell me how many sizes of bottled water \
                       you have. Do you know how big they are?
  - Are your mushrooms hallucinogenic?
  - What is the cost of a cheese pizza with sausage?
  - What is the price of a cheese pizza topped with sausage and mushroom?
     -- produces result but addings price of same topping twice to reach the wrong conclusion; data must not be in the best possible format. Would a better structure of the data make this foolproof?
     -- Repeating with a SQLDatabaseSequentialChain seems to produce correct answer for "What is the price of a small pizza topped with both sausage and mushroom?"
  - What is the diameter of each pizza size?
    -- the sequential chain knew that diameter was not provided, and it did not hallucinate any details.
    -- db chain made something up but then said this information is not available in the tables

Second try, separate items into tables per each type (pizzas, toppings, sides, drinks). Same schema (Item, Price large, Price medium, Price small). seems better. Bottled Water query worked. Number of pizzas and prices query worked.  Other examples?

This is for a single isolated query. Not a chatbot with message history. Not sure how to do that.

Discuss if model needs to see the data or not, and would this affect the output answers? direct=False?  This is concern for sensitive data. Hosting a model on premise could alleviate the issue so that no data ever gets out.

[examine if langchain db query of a more structured table is more robust. prototype table, have chatgpt write the query, then stuff the query results back into the prompt for the model to compose an answer to, as in the building systems example; difference here is that langchain db query is being used]

Agent feature in LangChain is one of the newer and most powerful features to the library. LLM as reasoning engine instead of just a Q&A tool.

LLM assisted QnA evaluation?  Example of using the LLM to help evaluate.  New process. Exciting times.

That the chatbot can reason in such detail about the menu, which is essentially a simple table, suggests the potential for reasoning about more complex table or database of tables. 



Does using real relational database tables somehow alleviate the hallucination problem? Does heavily structuring the data prevent wrong details from being made up, e.g., because the answer must be expressable as a valid sql query?

Suggestive that it can retrieve and summarize tablular data.

Show examples for pizza prompt.

Show examples where it is making things up when details are missing.


